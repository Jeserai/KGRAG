models:
  # Large Language Models for entity extraction and QA
  llama_3_2_1b_instruct:
    name: "meta-llama/Llama-3.2-1B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 8
    quantization: "4bit"  # Use 4-bit quantization to save memory
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_2_5_0_5b_instruct:
    name: "Qwen/Qwen2.5-0.5B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 16
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_2_5_vl_3b_instruct:
    name: "Qwen/Qwen2.5-VL-3B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 8
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_2_5_vl_32b_instruct:
    name: "Qwen/Qwen2.5-VL-32B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 2
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_2_vl_7b_instruct:
    name: "Qwen/Qwen2-VL-7B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 4
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  openvla_7b:
    name: "openvla/openvla-7b"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 4
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  llama_3_2v_11b_cot:
    name: "Xkev/Llama-3.2V-11B-cot"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 2
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true

  # Embedding models - encoder-only models for text embeddings
  qwen_3_embedding_0_6b:
    name: "Qwen/Qwen3-Embedding-0.6B"
    type: "embedding"
    device: "cuda"
    batch_size: 32
    max_seq_length: 512
    normalize_embeddings: true
    
  # Note: Since we don't have sentence-transformers models cached, we'll use Qwen embedding
  # For production use, consider downloading sentence-transformers models like:
  # - sentence-transformers/all-MiniLM-L6-v2
  # - sentence-transformers/all-mpnet-base-v2
  # - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Default configurations - using smaller, faster models
default:
  llm_model: "llama_3_2_1b_instruct"  # Fast 1B model for quick testing
  embedding_model: "qwen_3_embedding_0_6b"  # Qwen's encoder-only embedding model
  
# Performance settings
performance:
  enable_model_caching: true
  max_memory_gb: 16
  cpu_fallback: true
  enable_gradient_checkpointing: true
  
# Knowledge Graph settings
kg:
  max_entities_per_chunk: 20
  max_relationships_per_chunk: 30
  entity_confidence_threshold: 0.7
  relationship_confidence_threshold: 0.6
  
# Retrieval settings
retrieval:
  max_context_length: 4096
  max_retrieved_entities: 10
  max_graph_hops: 2
  similarity_threshold: 0.7 
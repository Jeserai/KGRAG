models:
  # Large Language Models for entity extraction and QA
  # Available on school server at /data/models/huggingface
  llama_3_2_3b_instruct:
    name: "/data/models/huggingface/meta-llama/Llama-3.2-3B-Instruct"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 8
    quantization: "4bit"  # Use 4-bit quantization to save memory
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  llama_3_70b:
    name: "/data/models/huggingface/meta-llama/Meta-Llama-3-70B"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 1  # Smaller batch size for 70B model
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_1_5_7b:
    name: "/data/models/huggingface/qwen/Qwen1.5-7B"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 4
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true
    
  qwen_1_5_32b:
    name: "/data/models/huggingface/qwen/Qwen1.5-32B"
    type: "llm"
    device: "cuda"
    max_length: 8192
    temperature: 0.1
    top_p: 0.9
    batch_size: 2  # Smaller batch size for 32B model
    quantization: "4bit"
    load_in_8bit: false
    load_in_4bit: true
    trust_remote_code: true

  # Embedding models - encoder-only models for text embeddings
  # Using Qwen1.5-7B for embeddings (can be used for both LLM and embedding tasks)
  qwen_1_5_7b_embedding:
    name: "/data/models/huggingface/qwen/Qwen1.5-7B"
    type: "embedding"
    device: "cuda"
    batch_size: 16
    max_seq_length: 512
    normalize_embeddings: true

# Default configurations - using 3B model for balanced performance
default:
  llm_model: "llama_3_2_3b_instruct"  # Good balance of performance and speed
  embedding_model: "qwen_1_5_7b_embedding"  # Using Qwen1.5-7B for embeddings
  
# Performance settings optimized for server cluster
performance:
  enable_model_caching: true
  max_memory_gb: 32  # Assuming server has more memory
  cpu_fallback: true
  enable_gradient_checkpointing: true
  enable_flash_attention: true  # Enable flash attention for better performance
  
# Knowledge Graph settings
kg:
  max_entities_per_chunk: 20
  max_relationships_per_chunk: 30
  entity_confidence_threshold: 0.7
  relationship_confidence_threshold: 0.6
  
# Retrieval settings
retrieval:
  max_context_length: 4096
  max_retrieved_entities: 10
  max_graph_hops: 2
  similarity_threshold: 0.7 